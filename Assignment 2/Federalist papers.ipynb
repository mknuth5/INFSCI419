{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Federalist papers**\n",
    "\n",
    "Alexander Hamilton, James Madison, or John Jay?  For more than 150 years, historians argued over the authorship of the 12 essays in _The Federalist Papers_. It wasn't until 1963 that the mystery was solved by Frederick Mosteller of Harvard University and David Wallace of the University of Chicago. [Nabokov's Favorite Word Is _Mauve_ by Ben Blatt]\n",
    "\n",
    "Full text of _The Federalist Papers_ is available at http://www.gutenberg.org/ebooks/1404"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to our data file (source file)\n",
    "source_file_name = 'federalist_papers.txt'\n",
    "try:\n",
    "    fed_papers_file = open(source_file_name, 'r')\n",
    "    all_text = fed_papers_file.read()\n",
    "except IOError:\n",
    "    print(\"Cannot open source file\")\n",
    "except:\n",
    "    print(\"An unknown error as occured\")\n",
    "\n",
    "# We can read all text at once\n",
    "\n",
    "#print(all_text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are a couple of ways we could find frequencies of the words \"while\" and \"whilst\".  \n",
    "# For now, let's convert our chunk of text into a list of words\n",
    "\n",
    "word_list = all_text.split(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Will this work?  Are words always separated by spaces?\n",
    "# While there are better methods for dealing with text parsing (for example, nltk toolkit)\n",
    "# for now we'll take care of things in a quick and dirty way\n",
    "\n",
    "punctuation_marks = ['!','.', ',', ':', ';', '?', '-', '\\n']\n",
    "for pm in punctuation_marks:\n",
    "    all_text = all_text.replace(pm, ' ')\n",
    "                     \n",
    "# print(all_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It would be a good idea to convert everything to lower case before we do anything else\n",
    "all_text = all_text.lower()\n",
    "\n",
    "# Now let's build a list of words\n",
    "word_list = all_text.split(\" \")\n",
    "# print(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#removes all stop words\n",
    "stop_words_file = \"common-english-words.txt\"\n",
    "try:\n",
    "    stop_words = open(stop_words_file, 'r')\n",
    "    all_stop_words = stop_words.read()\n",
    "except IOError:\n",
    "    print(\"Cannot open source file\")\n",
    "except:\n",
    "    print(\"An unknown error as occured\")\n",
    "stop_word_list = all_stop_words.split(\",\")\n",
    "stop_word_list = set(stop_word_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#goes through every word in stop list and then everyword in word list to remove all stop words\n",
    "for word in word_list:\n",
    "    if word in stop_word_list:\n",
    "        word_list.remove(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creates dictionary with each word and frequency            \n",
    "word_dict = {}\n",
    "for word in word_list:\n",
    "    if word in word_dict:\n",
    "        word_dict[word] += 1\n",
    "    else:\n",
    "        word_dict[word] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#takes in user input to find frequency\n",
    "input_word = input()\n",
    "if input_word == \"\":\n",
    "    print(\"Please input a valid word\")\n",
    "if input_word in stop_word_list:\n",
    "    print(\"This word is a stop word so it was removed. Please input a new word that is not: \" + str(sorted(stop_word_list)))\n",
    "#print(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"The word \" + str(input_word) + \" appears in The Federalist Papers: \" + str(word_dict[input_word]) + \" times.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: Why do we care about word frequencies?  Can you give me a use case with EMR data where this would be useful?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word frequencies can be used to determine who wrote something and where they are from.\n",
    "# For example, in the 1990s a criminal mastermind had been bombing buildings around the country.\n",
    "# He left no physical evidence except for manifestos he sent into prominent newspapers.\n",
    "# Using linguistics and word frequencies, the FBI was able to pinpoint the \"Unabombers\" identity as Ted Kaczynski."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
